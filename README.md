# American Sign Language Recognition and Conversion to Speech

![ASL Example]([https://github.com/harshils06/ASL-Recognition-and-Speech-Conversion/blob/main/models/result.jpg])

This project focuses on real-time recognition of American Sign Language (ASL) gestures using Convolutional Neural Networks (CNN), specifically Inception V3 architecture. The recognized gestures are then converted into spoken language. The project integrates OpenCV (cv2) for real-time gesture capturing from a camera feed.

## Features

- Real-time ASL gesture recognition using CNN, Inception V3.
- Conversion of recognized ASL gestures into spoken language.
- Integration of OpenCV (cv2) for capturing real-time gestures.
- User-friendly interface for capturing, recognizing, and converting gestures.
- Enhancing communication for the deaf and hard-of-hearing community.

## Installation

1. Clone the repository:
2. Run the .ipynb which will download a model.h5 file.
3. Create a folder named 'Model' and store the model.h5 file inside it.
4. Run the pred.py file
